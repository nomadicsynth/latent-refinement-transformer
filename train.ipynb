{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684adc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"./preprocessed_dataset_562_55\"  # Train: 10000 Test: 1000\n",
    "# dataset_name = \"./preprocessed_dataset_1108_113\"  # Train: 20000 Test: 2000\n",
    "# dataset_name = \"./preprocessed_dataset_2184_227\"  # Train: 40000 Test: 4000\n",
    "# dataset_name = \"./preprocessed_dataset_4360_446\"  # Train: 80000 Test: 8000\n",
    "# dataset_name = \"./preprocessed_dataset_8770_888\"  # Train: 160000 Test: 16000\n",
    "# dataset_name = \"./preprocessed_dataset_17510_1759\"  # Train: 320000 Test: 32000\n",
    "# dataset_name = \"./preprocessed_dataset_34954_3492\"  # Train: 640000 Test: 64000\n",
    "# dataset_name = \"./preprocessed_dataset_69668_7001\"  # Train: 1280000 Test: 128000\n",
    "dataset_name = \"./preprocessed_dataset_83208_9318\"  # Train: 1530000 Test: 170000 - Full dataset of 1.7M rows split at 10%\n",
    "\n",
    "tokenizer_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_config_name = tokenizer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = \"science-llm\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the English Wikipedia dataset from the latest dump\n",
    "dataset = load_from_disk(dataset_name, keep_in_memory=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff221e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokeniser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralConfig\n",
    "\n",
    "config = MistralConfig.from_pretrained(model_config_name)\n",
    "\n",
    "# Modify the config for 70M params\n",
    "config.hidden_size = 768\n",
    "config.intermediate_size = 3688\n",
    "config.num_hidden_layers = 2\n",
    "config.num_attention_heads = 8\n",
    "config.num_key_value_heads = 4\n",
    "config._attn_implementation = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb01484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "model = MistralForCausalLM(config).to(device=device, dtype=torch.bfloat16)\n",
    "\n",
    "trainable_params = model.num_parameters()\n",
    "trainable_params_hr = 0\n",
    "if trainable_params >= 1e9:\n",
    "    trainable_params_hr = f\"{trainable_params / 1e9:.0f}B\"\n",
    "elif trainable_params >= 1e6:\n",
    "    trainable_params_hr = f\"{trainable_params / 1e6:.0f}M\"\n",
    "elif trainable_params >= 1e3:\n",
    "    trainable_params_hr = f\"{trainable_params / 1e3:.0f}K\"\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print(f\"Trainable parameters: {trainable_params} ({trainable_params_hr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# torch._dynamo.config.capture_scalar_outputs = True\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_on_start=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    completion_only_loss=False,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    # activation_offloading=True,\n",
    "    max_length=32768,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    # report_to=\"none\",\n",
    "    dataset_num_proc=4,\n",
    "    eos_token=tokenizer.eos_token,\n",
    "    pad_token=tokenizer.pad_token,\n",
    "    # torch_compile=True,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\"skip_preprocessing\": True},\n",
    "    use_liger_kernel=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f53d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
